import{r as h,j as e,L as v}from"./index-quAWpIA7.js";import{C as y}from"./check-circle-CZEU1TAG.js";import{X as z}from"./x-circle-D3t0USxl.js";import{R as S}from"./refresh-cw-BoTFdhd7.js";const d=[{id:1,question:"Why do modern LLMs use subword tokenization instead of word-level?",options:["It's faster to compute","It can handle rare/unknown words by breaking them into known pieces","It produces fewer tokens","It requires less memory"],correct:1,explanation:'Subword tokenization handles Out-of-Vocabulary (OOV) words by breaking them into known subword units. For example, "unhappiness" becomes "un" + "happi" + "ness".'},{id:2,question:'What does the "##" prefix mean in WordPiece tokenization?',options:["It marks the start of a new word","It indicates a special token","It means this token continues the previous word (not a word start)","It represents a number"],correct:2,explanation:'In WordPiece, "##" indicates that this subword is a continuation of the previous token, not the start of a new word. E.g., "playing" ‚Üí ["play", "##ing"]'},{id:3,question:"In BPE, what determines which token pairs get merged?",options:["Alphabetical order","The length of the resulting token","Frequency - most common pairs are merged first","Random selection"],correct:2,explanation:"BPE merges the most frequently occurring adjacent token pairs. This creates a vocabulary of common subwords that efficiently represent the training corpus."},{id:4,question:"Which tokenization method does GPT-4 primarily use?",options:["Character-level tokenization","Word-level tokenization","BPE (Byte Pair Encoding)","WordPiece"],correct:2,explanation:"GPT models use BPE (specifically cl100k_base for GPT-4), which efficiently balances vocabulary size with the ability to represent any text."},{id:5,question:"What happens when a tokenizer encounters a word not in its vocabulary?",options:["It throws an error","It skips the word","It breaks the word into smaller known subwords or characters","It replaces it with a random word"],correct:2,explanation:"Subword tokenizers can represent ANY text by breaking unknown words into known subword pieces or even individual characters as a fallback."}],P=m=>{const o=[];return m.split(/(\s+)/).forEach(l=>{if(l.match(/^\s+$/))o.push({token:l===" "?"‚ñÅ":"‚ñÅ".repeat(l.length),type:"space"});else if(l.length<=3)o.push({token:l,type:"word"});else{const x=["ing","tion","ment","ness","ful","less","able","ly","ed","er","est","s","es"];let n=l,a=!0;const p=["un","re","pre","dis","mis","over","under"];for(const s of p)if(n.toLowerCase().startsWith(s)&&n.length>s.length+2){o.push({token:s,type:"prefix"}),n=n.slice(s.length),a=!1;break}let b=null;for(const s of x)if(n.toLowerCase().endsWith(s)&&n.length>s.length+1){b=s;break}if(b){const s=n.slice(0,-b.length);o.push({token:a?s:"##"+s,type:"stem"}),o.push({token:"##"+b,type:"suffix"})}else o.push({token:a?n:"##"+n,type:"word"})}}),o};function E(){const[m,o]=h.useState("quiz"),[c,l]=h.useState(0),[x,n]=h.useState(null),[a,p]=h.useState(!1),[b,s]=h.useState(0),[u,g]=h.useState("The unhappiness of tokenization is overwhelming"),[f,j]=h.useState([]);h.useEffect(()=>{j(P(u))},[u]);const w=t=>{n(t),p(!0),t===d[c].correct&&s(r=>r+1)},k=()=>{c<d.length-1&&(l(t=>t+1),n(null),p(!1))},N=()=>{l(0),n(null),p(!1),s(0)},i=d[c];return e.jsx("div",{className:"p-8 h-full",children:e.jsxs("div",{className:"max-w-4xl mx-auto",children:[e.jsxs("div",{className:"text-center mb-6",children:[e.jsx("h2",{className:"text-3xl font-bold text-indigo-900 mb-2",children:"Practice Lab"}),e.jsx("p",{className:"text-slate-800 dark:text-slate-600",children:"Test your understanding of tokenization"})]}),e.jsxs("div",{className:"flex justify-center gap-4 mb-8",children:[e.jsx("button",{onClick:()=>o("quiz"),className:`px-6 py-2 rounded-lg font-bold transition-all ${m==="quiz"?"bg-indigo-600 text-white":"bg-slate-100 text-slate-800 dark:text-slate-600 hover:bg-slate-200"}`,children:"üìù Quiz"}),e.jsx("button",{onClick:()=>o("sandbox"),className:`px-6 py-2 rounded-lg font-bold transition-all ${m==="sandbox"?"bg-indigo-600 text-white":"bg-slate-100 text-slate-800 dark:text-slate-600 hover:bg-slate-200"}`,children:"üß™ Sandbox"})]}),m==="quiz"?e.jsxs("div",{className:"bg-slate-50 rounded-xl p-6",children:[e.jsxs("div",{className:"flex justify-between items-center mb-4",children:[e.jsxs("span",{className:"text-sm text-slate-800 dark:text-slate-600",children:["Question ",c+1," of ",d.length]}),e.jsxs("span",{className:"text-sm font-bold text-indigo-600",children:["Score: ",b,"/",d.length]})]}),e.jsx("div",{className:"w-full bg-slate-200 rounded-full h-2 mb-6",children:e.jsx("div",{className:"bg-indigo-600 h-2 rounded-full transition-all",style:{width:`${(c+1)/d.length*100}%`}})}),e.jsxs("div",{className:"bg-white rounded-lg p-6 mb-4 border",children:[e.jsx("h3",{className:"text-lg font-bold text-slate-800 mb-4",children:i.question}),e.jsx("div",{className:"space-y-3",children:i.options.map((t,r)=>e.jsx("button",{onClick:()=>!a&&w(r),disabled:a,className:`w-full p-3 rounded-lg border-2 text-left transition-all ${a?r===i.correct?"bg-green-100 border-green-400":r===x?"bg-red-100 border-red-400":"bg-slate-50 border-slate-200":x===r?"bg-indigo-50 border-indigo-400":"bg-white border-slate-200 hover:border-indigo-300"}`,children:e.jsxs("div",{className:"flex items-center gap-3",children:[a&&r===i.correct&&e.jsx(y,{className:"text-green-600",size:20}),a&&r===x&&r!==i.correct&&e.jsx(z,{className:"text-red-600",size:20}),e.jsx("span",{children:t})]})},r))})]}),a&&e.jsx("div",{className:`p-4 rounded-lg mb-4 ${x===i.correct?"bg-green-50 border border-green-200":"bg-amber-50 border border-amber-200"}`,children:e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(v,{className:x===i.correct?"text-green-600":"text-amber-600",size:20}),e.jsx("p",{className:x===i.correct?"text-green-800":"text-amber-800",children:i.explanation})]})}),e.jsxs("div",{className:"flex justify-between",children:[e.jsxs("button",{onClick:N,className:"flex items-center gap-2 px-4 py-2 text-slate-800 dark:text-slate-600 hover:text-slate-800",children:[e.jsx(S,{size:16})," Reset"]}),a&&c<d.length-1&&e.jsx("button",{onClick:k,className:"px-6 py-2 bg-indigo-600 text-white rounded-lg font-bold hover:bg-indigo-700",children:"Next Question ‚Üí"}),a&&c===d.length-1&&e.jsxs("div",{className:"text-lg font-bold text-indigo-600",children:["Final Score: ",b,"/",d.length," üéâ"]})]})]}):e.jsxs("div",{className:"bg-slate-50 rounded-xl p-6",children:[e.jsx("h3",{className:"text-xl font-bold text-slate-800 mb-4",children:"Tokenization Sandbox"}),e.jsxs("div",{className:"mb-4",children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-2",children:"Enter text to tokenize:"}),e.jsx("textarea",{value:u,onChange:t=>g(t.target.value),className:"w-full p-3 border-2 border-slate-200 rounded-lg focus:border-indigo-500 resize-none",rows:3,placeholder:"Type something to see how it gets tokenized..."})]}),e.jsx("div",{className:"flex flex-wrap gap-2 mb-6",children:["unhappiness","tokenization is cool","GPT-4 loves embeddings","ÿßŸÑŸÖÿ±ÿ≠ÿ®ÿß","üéâ emoji test"].map(t=>e.jsx("button",{onClick:()=>g(t),className:"px-3 py-1 text-xs bg-white border rounded-full hover:bg-slate-100",children:t},t))}),e.jsxs("div",{className:"bg-white rounded-lg p-4 border",children:[e.jsxs("div",{className:"flex justify-between items-center mb-3",children:[e.jsxs("h4",{className:"font-bold text-slate-700",children:["Tokens (",f.length,")"]}),e.jsxs("span",{className:"text-xs text-slate-700 dark:text-slate-500",children:["~",(u.length/4).toFixed(0)," GPT tokens estimate"]})]}),e.jsx("div",{className:"flex flex-wrap gap-2",children:f.map((t,r)=>e.jsxs("div",{className:`px-3 py-1 rounded-lg font-mono text-sm flex items-center gap-1 ${t.type==="prefix"?"bg-orange-100 border-2 border-orange-300":t.type==="suffix"?"bg-purple-100 border-2 border-purple-300":t.type==="stem"?"bg-blue-100 border-2 border-blue-300":t.type==="space"?"bg-slate-100 border-2 border-slate-300":"bg-green-100 border-2 border-green-300"}`,children:[e.jsx("span",{className:"text-xs text-slate-700 dark:text-slate-500",children:r}),t.token]},r))})]}),e.jsxs("div",{className:"mt-4 flex flex-wrap gap-4 text-xs",children:[e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("span",{className:"w-4 h-4 bg-orange-100 border-2 border-orange-300 rounded"}),e.jsx("span",{children:"Prefix"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("span",{className:"w-4 h-4 bg-blue-100 border-2 border-blue-300 rounded"}),e.jsx("span",{children:"Stem"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("span",{className:"w-4 h-4 bg-purple-100 border-2 border-purple-300 rounded"}),e.jsx("span",{children:"Suffix (##)"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("span",{className:"w-4 h-4 bg-green-100 border-2 border-green-300 rounded"}),e.jsx("span",{children:"Word"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("span",{className:"w-4 h-4 bg-slate-100 border-2 border-slate-300 rounded"}),e.jsx("span",{children:"Space"})]})]})]})]})})}export{E as default};
