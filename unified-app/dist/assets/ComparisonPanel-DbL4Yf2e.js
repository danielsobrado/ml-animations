import{r as o,j as e,Z as d,T as l}from"./index-BmgLEcVh.js";import{C as a}from"./check-circle-sA13bXaK.js";function h(){const[i,c]=o.useState("training"),r={training:{title:"Training Approach",glove:{items:["Pre-computes co-occurrence matrix","Weighted least squares optimization","Uses all co-occurrence statistics at once","Trains on observed co-occurrences (X_ij > 0)"]},word2vec:{items:["Trains with sliding context window","Stochastic gradient descent","Samples mini-batches from corpus","Uses negative sampling for efficiency"]}},objective:{title:"Learning Objective",glove:{items:["Minimize squared error on log counts","J = Î£ f(X_ij)(wÂ·wÌƒ + b - log X)Â²","Global co-occurrence statistics","Matrix factorization perspective"]},word2vec:{items:["Maximize probability of context words","Skip-gram: P(context | center)","CBOW: P(center | context)","Local context prediction task"]}},efficiency:{title:"Computational Efficiency",glove:{items:["One-time matrix construction","Training time: O(|X|) non-zero entries","Parallelizes well on non-zero entries","Memory: stores sparse co-occurrence matrix"]},word2vec:{items:["Multiple passes over corpus","Training time: O(corpus size Ã— epochs)","Parallelizes with async SGD","Memory: stores word vectors only"]}},performance:{title:"Performance Characteristics",glove:{items:["Better on word analogy tasks","Captures global statistics well","More interpretable objective","Faster training on large corpora"]},word2vec:{items:["Better on similarity tasks","Captures local context patterns","More flexible architecture","Online learning possible"]}}},n=["Both produce dense word vectors (50-300 dimensions)","Both capture semantic relationships","Both support word arithmetic (king - man + woman â‰ˆ queen)","Both use context window concept","Both pre-trained models widely available","Both foundational for downstream NLP tasks"];return e.jsxs("div",{className:"space-y-6 pb-20",children:[e.jsxs("div",{className:"text-center",children:[e.jsxs("h2",{className:"text-3xl font-bold mb-2",children:[e.jsx("span",{className:"gradient-text",children:"GloVe vs Word2Vec:"})," A Comparison"]}),e.jsx("p",{className:"text-gray-400",children:"Understanding the differences between two influential embedding methods"})]}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-violet-900/30 to-violet-800/20 rounded-xl p-6 border border-violet-500/30",children:[e.jsxs("div",{className:"flex items-center gap-3 mb-4",children:[e.jsx("span",{className:"text-3xl",children:"ðŸ”®"}),e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-bold text-violet-400",children:"GloVe"}),e.jsx("p",{className:"text-sm text-gray-400",children:"Global Vectors"})]})]}),e.jsxs("p",{className:"text-gray-300 text-sm",children:[e.jsx("strong",{className:"text-violet-400",children:"Count-based method"})," that learns from the global word-word co-occurrence matrix. Combines the benefits of global matrix factorization and local context window methods."]}),e.jsx("div",{className:"mt-4 text-xs text-gray-500",children:"Stanford NLP Group, 2014"})]}),e.jsxs("div",{className:"bg-gradient-to-br from-cyan-900/30 to-cyan-800/20 rounded-xl p-6 border border-cyan-500/30",children:[e.jsxs("div",{className:"flex items-center gap-3 mb-4",children:[e.jsx("span",{className:"text-3xl",children:"ðŸŽ¯"}),e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-bold text-cyan-400",children:"Word2Vec"}),e.jsx("p",{className:"text-sm text-gray-400",children:"Skip-gram / CBOW"})]})]}),e.jsxs("p",{className:"text-gray-300 text-sm",children:[e.jsx("strong",{className:"text-cyan-400",children:"Prediction-based method"})," that learns word embeddings by predicting context words (Skip-gram) or center words (CBOW) using a neural network trained with SGD."]}),e.jsx("div",{className:"mt-4 text-xs text-gray-500",children:"Google, Mikolov et al., 2013"})]})]}),e.jsx("div",{className:"flex flex-wrap justify-center gap-2",children:Object.keys(r).map(t=>e.jsx("button",{onClick:()=>c(t),className:`px-4 py-2 rounded-lg text-sm font-medium transition-all ${i===t?"bg-violet-600 text-white":"bg-white/10 text-gray-400 hover:bg-white/20"}`,children:r[t].title},t))}),e.jsxs("div",{className:"bg-black/30 rounded-xl p-6 border border-white/10",children:[e.jsx("h4",{className:"text-xl font-bold text-white mb-6 text-center",children:r[i].title}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"bg-violet-900/20 rounded-lg p-4 border border-violet-500/30",children:[e.jsx("h5",{className:"text-violet-400 font-bold mb-4 flex items-center gap-2",children:"ðŸ”® GloVe"}),e.jsx("ul",{className:"space-y-3",children:r[i].glove.items.map((t,s)=>e.jsxs("li",{className:"flex items-start gap-2 text-gray-300 text-sm",children:[e.jsx(a,{size:16,className:"text-violet-400 mt-0.5 flex-shrink-0"}),t]},s))})]}),e.jsxs("div",{className:"bg-cyan-900/20 rounded-lg p-4 border border-cyan-500/30",children:[e.jsx("h5",{className:"text-cyan-400 font-bold mb-4 flex items-center gap-2",children:"ðŸŽ¯ Word2Vec"}),e.jsx("ul",{className:"space-y-3",children:r[i].word2vec.items.map((t,s)=>e.jsxs("li",{className:"flex items-start gap-2 text-gray-300 text-sm",children:[e.jsx(a,{size:16,className:"text-cyan-400 mt-0.5 flex-shrink-0"}),t]},s))})]})]})]}),e.jsxs("div",{className:"bg-black/30 rounded-xl p-6 border border-white/10 overflow-x-auto",children:[e.jsx("h4",{className:"text-lg font-bold text-violet-400 mb-4",children:"ðŸ“Š Quick Comparison Table"}),e.jsxs("table",{className:"w-full text-sm",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"border-b border-white/20",children:[e.jsx("th",{className:"text-left py-2 px-3 text-gray-400",children:"Aspect"}),e.jsx("th",{className:"py-2 px-3 text-violet-400",children:"GloVe"}),e.jsx("th",{className:"py-2 px-3 text-cyan-400",children:"Word2Vec"})]})}),e.jsxs("tbody",{className:"divide-y divide-white/10",children:[e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Approach"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Count-based"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Prediction-based"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Statistics"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Global"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Local"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Optimization"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Weighted Least Squares"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"SGD + Negative Sampling"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Pre-processing"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Build co-occurrence matrix"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"None (online)"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Memory Usage"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Higher (stores matrix)"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Lower"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Training Speed"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Faster on large corpora"}),e.jsx("td",{className:"py-3 px-3 text-center text-gray-400",children:"Good for medium corpora"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"py-3 px-3 text-gray-300",children:"Incremental Learning"}),e.jsx("td",{className:"py-3 px-3 text-center text-red-400",children:"âŒ No"}),e.jsx("td",{className:"py-3 px-3 text-center text-green-400",children:"âœ… Yes"})]})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-r from-green-900/20 to-cyan-900/20 rounded-xl p-6 border border-green-500/30",children:[e.jsxs("h4",{className:"flex items-center gap-2 text-lg font-bold text-green-400 mb-4",children:[e.jsx(d,{size:20}),"What They Have in Common"]}),e.jsx("div",{className:"grid md:grid-cols-2 gap-3",children:n.map((t,s)=>e.jsxs("div",{className:"flex items-start gap-2 text-gray-300 text-sm",children:[e.jsx(a,{size:16,className:"text-green-400 mt-0.5 flex-shrink-0"}),t]},s))})]}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"bg-violet-900/20 rounded-xl p-6 border border-violet-500/30",children:[e.jsxs("h4",{className:"flex items-center gap-2 text-lg font-bold text-violet-400 mb-4",children:[e.jsx(l,{size:20}),"Use GloVe When:"]}),e.jsx("ul",{className:"space-y-2",children:["You have a large, fixed corpus","Word analogy tasks are important","Training time is a concern","Global statistics matter for your task","Using pre-trained embeddings"].map((t,s)=>e.jsxs("li",{className:"flex items-start gap-2 text-gray-300 text-sm",children:[e.jsx(a,{size:16,className:"text-violet-400 mt-0.5 flex-shrink-0"}),t]},s))})]}),e.jsxs("div",{className:"bg-cyan-900/20 rounded-xl p-6 border border-cyan-500/30",children:[e.jsxs("h4",{className:"flex items-center gap-2 text-lg font-bold text-cyan-400 mb-4",children:[e.jsx(l,{size:20}),"Use Word2Vec When:"]}),e.jsx("ul",{className:"space-y-2",children:["You need online/incremental learning","Corpus keeps growing over time","Memory is limited","Word similarity is the main task","You want more control over training"].map((t,s)=>e.jsxs("li",{className:"flex items-start gap-2 text-gray-300 text-sm",children:[e.jsx(a,{size:16,className:"text-cyan-400 mt-0.5 flex-shrink-0"}),t]},s))})]})]}),e.jsxs("div",{className:"bg-black/30 rounded-xl p-6 border border-white/10 text-center",children:[e.jsx("h4",{className:"text-lg font-bold text-white mb-3",children:"ðŸŽ“ The Practical Truth"}),e.jsxs("p",{className:"text-gray-300 max-w-2xl mx-auto",children:["In practice, both methods produce comparable results on most tasks. The choice often comes down to ",e.jsx("strong",{className:"text-violet-400",children:"availability of pre-trained vectors"})," or",e.jsx("strong",{className:"text-cyan-400",children:" specific task requirements"}),". Today, both are often superseded by contextual embeddings (BERT, GPT) for state-of-the-art results."]})]})]})}export{h as default};
