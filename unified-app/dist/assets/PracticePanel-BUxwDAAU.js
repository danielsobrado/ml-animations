import{r as c,j as e,o as v}from"./index-BmgLEcVh.js";import{T as N}from"./trophy-JgKQIZ3q.js";import{R as k}from"./rotate-ccw-DnC3UZWM.js";import{C as u}from"./check-circle-sA13bXaK.js";import{X as g}from"./x-circle-D2E2srla.js";import{H as W}from"./help-circle-D0inx9Qu.js";function O(){const[r,p]=c.useState(0),[n,d]=c.useState(null),[i,m]=c.useState(!1),[l,h]=c.useState(0),[o,x]=c.useState([]),t=[{type:"multiple",question:"What is the main idea behind Word2Vec?",options:["Words that appear in similar contexts have similar meanings","Words should be represented as one-hot vectors","All words are equally different from each other","Word order is the most important feature"],correct:0,explanation:'Word2Vec is based on the distributional hypothesis: "You shall know a word by the company it keeps." Words appearing in similar contexts tend to have similar vector representations.'},{type:"multiple",question:"In Skip-gram, what is the input and what is the output?",options:["Input: context words, Output: center word","Input: center word, Output: context words","Input: sentence, Output: document vector","Input: one-hot vector, Output: TF-IDF vector"],correct:1,explanation:"Skip-gram takes a center word as input and tries to predict the surrounding context words. This is the opposite of CBOW."},{type:"multiple",question:"What is the purpose of negative sampling?",options:["To increase vocabulary size","To make training faster by avoiding full softmax","To generate negative reviews","To remove stop words"],correct:1,explanation:"Negative sampling converts the expensive softmax over the entire vocabulary into a binary classification task, making training much faster (O(V) → O(k))."},{type:"multiple",question:'What does "king - man + woman ≈ queen" demonstrate?',options:["Word2Vec cannot do arithmetic","Word vectors capture semantic relationships","All words have the same vector","One-hot encoding is better than embeddings"],correct:1,explanation:"This famous analogy shows that Word2Vec vectors capture semantic relationships. The gender relationship is encoded as a consistent direction in the embedding space."},{type:"multiple",question:"Which architecture is typically faster to train?",options:["Skip-gram","CBOW","Both are equally fast","Neither uses neural networks"],correct:1,explanation:"CBOW is typically faster because it makes one prediction per context window, while Skip-gram makes multiple predictions (one for each context word)."},{type:"multiple",question:"What is the typical dimensionality of Word2Vec embeddings?",options:["10-50 dimensions","100-300 dimensions","1000-10000 dimensions","Vocabulary size"],correct:1,explanation:"Word2Vec embeddings are typically 100-300 dimensions. This is much smaller than one-hot vectors (vocabulary size) but rich enough to capture semantic relationships."},{type:"multiple",question:"Which metric is commonly used to measure word similarity in embedding space?",options:["Euclidean distance","Manhattan distance","Cosine similarity","Hamming distance"],correct:2,explanation:"Cosine similarity measures the angle between vectors, which captures semantic similarity regardless of vector magnitude. Values range from -1 to 1."},{type:"multiple",question:"What is a major limitation of Word2Vec?",options:["It uses too much memory","Each word has only one vector (no context awareness)","It can only work with English","It requires labeled training data"],correct:1,explanation:'Word2Vec assigns a single static vector to each word, so "bank" has the same vector whether it means riverbank or financial bank. This is solved by contextual embeddings like BERT.'},{type:"multiple",question:"In negative sampling, how are negative samples typically selected?",options:["Completely randomly","Based on word frequency raised to 0.75 power","Only stop words","Words with opposite meaning"],correct:1,explanation:"Negative samples are drawn from a distribution proportional to word frequency^0.75. The 0.75 power smooths the distribution, giving rare words more chance to be selected."},{type:"multiple",question:'What determines the "context" in Word2Vec training?',options:["The entire document","Words within a fixed window size","Only the immediately adjacent words","All words in the corpus"],correct:1,explanation:"Context is defined by a window size parameter (typically 5-10). Words within this window of the center word are considered context words for training."}],b=a=>{if(i)return;d(a),m(!0);const s=a===t[r].correct;s&&h(j=>j+1),x([...o,{question:r,correct:s}])},f=()=>{r<t.length-1&&(p(a=>a+1),d(null),m(!1))},w=()=>{p(0),d(null),m(!1),h(0),x([])},y=o.length===t.length;return e.jsxs("div",{className:"space-y-6 pb-20",children:[e.jsxs("div",{className:"text-center",children:[e.jsxs("h2",{className:"text-3xl font-bold mb-2",children:[e.jsx("span",{className:"text-pink-400",children:"Practice"})," Quiz"]}),e.jsx("p",{className:"text-gray-400",children:"Test your understanding of Word2Vec"})]}),e.jsxs("div",{className:"bg-black/30 rounded-xl p-4 border border-white/10",children:[e.jsxs("div",{className:"flex justify-between items-center mb-2",children:[e.jsx("span",{className:"text-sm text-gray-400",children:"Progress"}),e.jsxs("span",{className:"text-sm text-pink-400",children:[o.length," / ",t.length]})]}),e.jsx("div",{className:"flex gap-1",children:t.map((a,s)=>e.jsx("div",{className:`h-2 flex-1 rounded-full transition-all ${o[s]!==void 0?o[s].correct?"bg-green-500":"bg-red-500":s===r?"bg-pink-500":"bg-white/10"}`},s))}),e.jsxs("div",{className:"flex justify-between items-center mt-2",children:[e.jsx("span",{className:"text-sm text-gray-400",children:"Score"}),e.jsxs("span",{className:"text-sm text-green-400",children:[l," correct"]})]})]}),y?e.jsxs("div",{className:"bg-gradient-to-r from-pink-900/30 to-purple-900/30 rounded-2xl p-8 border border-pink-500/30 text-center",children:[e.jsx(N,{size:64,className:"mx-auto text-yellow-400 mb-4"}),e.jsx("h3",{className:"text-2xl font-bold text-white mb-2",children:"Quiz Complete!"}),e.jsxs("p",{className:"text-4xl font-bold text-pink-400 mb-4",children:[l," / ",t.length]}),e.jsx("p",{className:"text-gray-400 mb-6",children:l===t.length?"Perfect score! You've mastered Word2Vec!":l>=t.length*.7?"Great job! You have a solid understanding.":"Keep practicing! Review the concepts and try again."}),e.jsxs("button",{onClick:w,className:"flex items-center gap-2 mx-auto px-6 py-3 bg-pink-600 hover:bg-pink-700 rounded-lg transition-colors",children:[e.jsx(k,{size:18}),"Try Again"]})]}):e.jsxs(e.Fragment,{children:[e.jsxs("div",{className:"bg-black/30 rounded-2xl p-6 border border-white/10",children:[e.jsx("div",{className:"flex items-center gap-2 mb-4",children:e.jsxs("span",{className:"px-3 py-1 bg-pink-600 rounded-full text-sm",children:["Question ",r+1]})}),e.jsx("h3",{className:"text-xl font-medium text-white mb-6",children:t[r].question}),e.jsx("div",{className:"grid gap-3",children:t[r].options.map((a,s)=>e.jsx("button",{onClick:()=>b(s),disabled:i,className:`p-4 rounded-lg text-left transition-all border ${i?s===t[r].correct?"bg-green-900/30 border-green-500 text-green-400":s===n?"bg-red-900/30 border-red-500 text-red-400":"bg-white/5 border-white/10 text-gray-500":"bg-white/5 border-white/10 hover:bg-white/10 hover:border-white/20 text-white"}`,children:e.jsxs("div",{className:"flex items-center gap-3",children:[e.jsx("span",{className:`w-8 h-8 rounded-full flex items-center justify-center text-sm font-medium ${i?s===t[r].correct?"bg-green-500 text-black":s===n?"bg-red-500 text-white":"bg-white/10":"bg-white/10"}`,children:i&&s===t[r].correct?e.jsx(u,{size:18}):i&&s===n?e.jsx(g,{size:18}):String.fromCharCode(65+s)}),e.jsx("span",{children:a})]})},s))})]}),i&&e.jsx("div",{className:`rounded-xl p-4 border ${n===t[r].correct?"bg-green-900/20 border-green-500/30":"bg-red-900/20 border-red-500/30"}`,children:e.jsxs("div",{className:"flex items-start gap-3",children:[n===t[r].correct?e.jsx(u,{className:"text-green-400 mt-1 flex-shrink-0",size:20}):e.jsx(g,{className:"text-red-400 mt-1 flex-shrink-0",size:20}),e.jsxs("div",{children:[e.jsx("p",{className:`font-medium ${n===t[r].correct?"text-green-400":"text-red-400"}`,children:n===t[r].correct?"Correct!":"Not quite right"}),e.jsx("p",{className:"text-gray-300 mt-1 text-sm",children:t[r].explanation})]})]})}),i&&r<t.length-1&&e.jsx("div",{className:"flex justify-center",children:e.jsxs("button",{onClick:f,className:"flex items-center gap-2 px-6 py-3 bg-pink-600 hover:bg-pink-700 rounded-lg transition-colors",children:["Next Question",e.jsx(v,{size:18})]})})]}),e.jsxs("div",{className:"bg-gradient-to-r from-pink-900/20 to-purple-900/20 rounded-xl p-6 border border-pink-500/30",children:[e.jsxs("h4",{className:"flex items-center gap-2 font-bold text-pink-400 mb-4",children:[e.jsx(W,{size:18}),"Quick Reference"]}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-4 text-sm",children:[e.jsxs("div",{className:"bg-black/30 rounded-lg p-3",children:[e.jsx("p",{className:"text-green-400 font-medium mb-1",children:"Skip-gram"}),e.jsx("p",{className:"text-gray-400",children:"Center word → Context words"})]}),e.jsxs("div",{className:"bg-black/30 rounded-lg p-3",children:[e.jsx("p",{className:"text-yellow-400 font-medium mb-1",children:"CBOW"}),e.jsx("p",{className:"text-gray-400",children:"Context words → Center word"})]}),e.jsxs("div",{className:"bg-black/30 rounded-lg p-3",children:[e.jsx("p",{className:"text-orange-400 font-medium mb-1",children:"Negative Sampling"}),e.jsx("p",{className:"text-gray-400",children:"Binary classification instead of softmax"})]}),e.jsxs("div",{className:"bg-black/30 rounded-lg p-3",children:[e.jsx("p",{className:"text-purple-400 font-medium mb-1",children:"Embedding Dimension"}),e.jsx("p",{className:"text-gray-400",children:"Typically 100-300"})]})]})]})]})}export{O as default};
