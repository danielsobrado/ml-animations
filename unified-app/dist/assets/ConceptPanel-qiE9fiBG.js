import{r as d,j as e,H as p,e as u,w as h,L as g}from"./index-CsxpOLHd.js";import{T as f}from"./type-DpaVq8o2.js";const j=[{text:"Hello world!",description:"Simple greeting"},{text:"I'm learning tokenization",description:"Contractions & long words"},{text:"GPT-4 is amazing!",description:"Hyphenated terms"},{text:"The quick brown fox jumps",description:"Common English"},{text:"日本語テスト",description:"Japanese (Unicode)"}],N=a=>a.split(""),y=a=>a.split(/(\s+|[.,!?;:'"()-])/).filter(l=>l.trim()),k=a=>{const l=[];return a.split(/(\s+)/).forEach(t=>{if(t.match(/^\s+$/))l.push(t);else if(t.length<=3)l.push(t);else{const c=["ing","tion","ed","er","est","ly","ment","ness","ize","ful"];let o=t,x=!1;for(const n of c)if(o.toLowerCase().endsWith(n)&&o.length>n.length+2){l.push(o.slice(0,-n.length)),l.push("##"+n),x=!0;break}x||(t.length>6?(l.push(t.slice(0,Math.ceil(t.length/2))),l.push("##"+t.slice(Math.ceil(t.length/2)))):l.push(t))}}),l.filter(t=>t)},b=["bg-blue-100 border-blue-300 text-blue-800","bg-green-100 border-green-300 text-green-800","bg-purple-100 border-purple-300 text-purple-800","bg-orange-100 border-orange-300 text-orange-800","bg-pink-100 border-pink-300 text-pink-800","bg-yellow-100 border-yellow-300 text-yellow-800","bg-cyan-100 border-cyan-300 text-cyan-800","bg-red-100 border-red-300 text-red-800"];function T(){const[a,l]=d.useState("Hello, I'm learning tokenization!"),[i,t]=d.useState("subword"),[c,o]=d.useState([]),[x,n]=d.useState(-1);return d.useEffect(()=>{let s;switch(i){case"char":s=N(a);break;case"word":s=y(a);break;case"subword":default:s=k(a);break}o(s),n(-1),s.forEach((r,m)=>{setTimeout(()=>n(m),m*100)})},[a,i]),e.jsx("div",{className:"p-8 h-full",children:e.jsxs("div",{className:"max-w-4xl mx-auto",children:[e.jsxs("div",{className:"text-center mb-8",children:[e.jsx("h2",{className:"text-3xl font-bold text-indigo-900 mb-4",children:"What is Tokenization?"}),e.jsxs("p",{className:"text-lg text-slate-700 leading-relaxed max-w-2xl mx-auto",children:["Tokenization is the process of breaking text into smaller pieces called ",e.jsx("strong",{children:"tokens"}),". These tokens are the fundamental units that language models understand and process."]})]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-4 mb-8",children:[e.jsxs("div",{className:"bg-blue-50 p-5 rounded-xl border-2 border-blue-100",children:[e.jsx("div",{className:"bg-blue-100 w-10 h-10 rounded-full flex items-center justify-center mb-3 text-blue-600",children:e.jsx(f,{size:20})}),e.jsx("h3",{className:"font-bold text-blue-900 mb-2",children:"Text → Numbers"}),e.jsx("p",{className:"text-blue-800 text-sm",children:"Computers can't read text directly. Tokens are mapped to numerical IDs that models can process."})]}),e.jsxs("div",{className:"bg-green-50 p-5 rounded-xl border-2 border-green-100",children:[e.jsx("div",{className:"bg-green-100 w-10 h-10 rounded-full flex items-center justify-center mb-3 text-green-600",children:e.jsx(p,{size:20})}),e.jsx("h3",{className:"font-bold text-green-900 mb-2",children:"Vocabulary Size"}),e.jsx("p",{className:"text-green-800 text-sm",children:"A fixed vocabulary of tokens (e.g., 50,000) can represent any text, balancing coverage and efficiency."})]}),e.jsxs("div",{className:"bg-purple-50 p-5 rounded-xl border-2 border-purple-100",children:[e.jsx("div",{className:"bg-purple-100 w-10 h-10 rounded-full flex items-center justify-center mb-3 text-purple-600",children:e.jsx(u,{size:20})}),e.jsx("h3",{className:"font-bold text-purple-900 mb-2",children:"Subword Magic"}),e.jsx("p",{className:"text-purple-800 text-sm",children:'Subword tokenization handles rare words by breaking them into known pieces: "unhappy" → "un" + "happy"'})]})]}),e.jsxs("div",{className:"bg-slate-50 rounded-xl p-6 mb-8",children:[e.jsx("h3",{className:"text-xl font-bold text-slate-800 mb-4",children:"Try It Yourself"}),e.jsxs("div",{className:"mb-4",children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-2",children:"Enter text:"}),e.jsx("input",{type:"text",value:a,onChange:s=>l(s.target.value),className:"w-full p-3 border-2 border-slate-200 rounded-lg focus:border-indigo-500 focus:ring-2 focus:ring-indigo-200 transition-all",placeholder:"Type something..."})]}),e.jsx("div",{className:"flex flex-wrap gap-2 mb-4",children:j.map((s,r)=>e.jsxs("button",{onClick:()=>l(s.text),className:"px-3 py-1 text-xs bg-white border border-slate-200 rounded-full hover:bg-slate-100 transition-colors",title:s.description,children:[s.text.slice(0,15),s.text.length>15?"...":""]},r))}),e.jsx("div",{className:"flex gap-4 mb-6",children:[{id:"char",label:"Character-level",desc:"Every character is a token"},{id:"word",label:"Word-level",desc:"Split on spaces/punctuation"},{id:"subword",label:"Subword (BPE)",desc:"Balanced approach used by GPT"}].map(s=>e.jsxs("label",{className:`flex-1 p-3 rounded-lg border-2 cursor-pointer transition-all ${i===s.id?"border-indigo-500 bg-indigo-50":"border-slate-200 hover:border-slate-300"}`,children:[e.jsx("input",{type:"radio",name:"method",value:s.id,checked:i===s.id,onChange:r=>t(r.target.value),className:"sr-only"}),e.jsx("div",{className:"font-bold text-slate-800",children:s.label}),e.jsx("div",{className:"text-xs text-slate-600",children:s.desc})]},s.id))}),e.jsxs("div",{className:"bg-white rounded-lg p-4 border border-slate-200",children:[e.jsxs("div",{className:"flex items-center gap-2 mb-3",children:[e.jsx("span",{className:"text-sm font-medium text-slate-600",children:"Original:"}),e.jsx("span",{className:"font-mono bg-slate-100 px-2 py-1 rounded",children:a})]}),e.jsxs("div",{className:"flex items-center gap-2 mb-3",children:[e.jsx(h,{className:"text-slate-400",size:20}),e.jsxs("span",{className:"text-sm font-medium text-slate-600",children:["Tokens (",c.length,"):"]})]}),e.jsx("div",{className:"flex flex-wrap gap-2",children:c.map((s,r)=>e.jsxs("span",{className:`inline-flex items-center px-3 py-1 rounded-lg border-2 font-mono text-sm transition-all duration-300 ${b[r%b.length]} ${r<=x?"opacity-100 scale-100":"opacity-0 scale-75"}`,children:[e.jsx("span",{className:"opacity-50 mr-1 text-xs",children:r}),s===" "?"␣":s]},r))})]})]}),e.jsxs("div",{className:"bg-amber-50 p-4 rounded-xl border border-amber-200 flex items-start gap-3",children:[e.jsx(g,{className:"text-amber-600 flex-shrink-0 mt-1",size:24}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-bold text-amber-900 mb-1",children:"Key Insight"}),e.jsxs("p",{className:"text-amber-800 text-sm",children:["Different tokenization methods produce different numbers of tokens for the same text.",e.jsx("strong",{children:" Subword tokenization"})," (like BPE) is the most common in modern LLMs because it balances vocabulary size with the ability to handle rare or novel words."]})]})]})]})})}export{T as default};
