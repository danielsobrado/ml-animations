import{r as i,j as e,L as M,m as E,g as I,Z as G}from"./index-quAWpIA7.js";import{C as W}from"./check-circle-2-D_aIm6E6.js";import{X as D}from"./x-circle-D3t0USxl.js";const r=[{id:1,question:"What is the main advantage of Parameter-Efficient Fine-Tuning (PEFT)?",options:["It makes the model more accurate","It updates only a small fraction of parameters, saving memory and compute","It speeds up inference time","It removes the need for training data"],correct:1,explanation:"PEFT methods like LoRA update only a tiny fraction of parameters (often <1%) while keeping the rest frozen, dramatically reducing memory and compute requirements."},{id:2,question:"In LoRA, what do the matrices A and B represent?",options:["The original weight matrix split in half","Low-rank decomposition matrices that approximate the weight update","Gradient accumulation buffers","Activation caches"],correct:1,explanation:"LoRA learns two small matrices B (dÃ—r) and A (rÃ—k) whose product BA approximates the weight update Î”W, where r is much smaller than d or k."},{id:3,question:'What does the "rank" (r) parameter in LoRA control?',options:["The learning rate","The batch size","The capacity/expressiveness of the adaptation","The number of layers to fine-tune"],correct:2,explanation:"The rank r controls how expressive the adaptation can be. Higher rank = more parameters = more capacity to learn complex adaptations, but also more memory."},{id:4,question:"What makes QLoRA different from standard LoRA?",options:["It uses larger rank values","It quantizes the base model to 4-bit precision","It removes the need for adapters","It fine-tunes all parameters"],correct:1,explanation:"QLoRA combines LoRA with 4-bit quantization of the base model (using NF4 format), reducing memory requirements by ~4x while maintaining similar performance."},{id:5,question:"After training, what can be done with LoRA weights for faster inference?",options:["They must be discarded","They can be merged into the base model weights","They need separate GPU memory","They require retraining"],correct:1,explanation:"LoRA weights can be merged into the base model (W' = W + BA) after training, resulting in zero additional inference latency compared to the original model."}];function X(){const[b,N]=i.useState("quiz"),[l,y]=i.useState(0),[w,k]=i.useState({}),[R,z]=i.useState(!1),[B,g]=i.useState(null),[d,j]=i.useState(!1),[s,o]=i.useState({modelSize:7,hiddenDim:4096,numLayers:32,rank:8,targetModules:2,precision:16,quantization:"none"}),T=t=>{g(t),j(!0),k({...w,[l]:t})},q=()=>{l<r.length-1?(y(l+1),g(null),j(!1)):z(!0)},S=()=>{y(0),k({}),z(!1),g(null),j(!1)},f=()=>{let t=0;return Object.entries(w).forEach(([a,n])=>{r[parseInt(a)].correct===n&&t++}),t},c=(()=>{const{modelSize:t,hiddenDim:a,numLayers:n,rank:m,targetModules:x,precision:A,quantization:L}=s;let h=t*1e9*(A/8);L==="8bit"&&(h=t*1e9*1),L==="4bit"&&(h=t*1e9*.5);const v=2*a*m*x*n,p=v*(A/8),C=p*3,P=p,Q=t*.5*1e9,F=h+p+C+P+Q;return{baseModel:(h/1e9).toFixed(2),loraParams:v,loraMemory:(p/1e6).toFixed(2),optimizer:(C/1e6).toFixed(2),totalTraining:(F/1e9).toFixed(2),percentTrainable:(v/(t*1e9)*100).toFixed(4)}})(),u=r[l];return e.jsx("div",{className:"p-6 h-full overflow-y-auto",children:e.jsxs("div",{className:"max-w-4xl mx-auto",children:[e.jsxs("div",{className:"flex justify-center gap-4 mb-6",children:[e.jsx("button",{onClick:()=>N("quiz"),className:`px-6 py-2 rounded-lg font-medium transition-all ${b==="quiz"?"bg-purple-500 text-white":"bg-white border hover:bg-slate-50"}`,children:"ðŸ“ Quiz"}),e.jsx("button",{onClick:()=>N("calculator"),className:`px-6 py-2 rounded-lg font-medium transition-all ${b==="calculator"?"bg-purple-500 text-white":"bg-white border hover:bg-slate-50"}`,children:"ðŸ§® LoRA Calculator"})]}),b==="quiz"?e.jsx("div",{className:"bg-white rounded-xl p-6 shadow-lg border",children:R?e.jsxs("div",{className:"text-center",children:[e.jsx("div",{className:"text-6xl mb-4",children:f()===r.length?"ðŸ†":f()>=3?"â­":"ðŸ“š"}),e.jsx("h3",{className:"text-2xl font-bold text-slate-800 mb-2",children:"Quiz Complete!"}),e.jsxs("p",{className:"text-xl text-slate-800 dark:text-slate-600 mb-6",children:["You scored ",e.jsx("span",{className:"font-bold text-purple-600",children:f()})," out of ",r.length]}),e.jsxs("button",{onClick:S,className:"flex items-center gap-2 mx-auto px-6 py-3 bg-purple-500 text-white rounded-lg hover:bg-purple-600",children:[e.jsx(E,{size:18}),"Try Again"]})]}):e.jsxs(e.Fragment,{children:[e.jsxs("div",{className:"flex justify-between items-center mb-6",children:[e.jsxs("span",{className:"text-sm text-slate-700 dark:text-slate-500",children:["Question ",l+1," of ",r.length]}),e.jsx("div",{className:"flex gap-1",children:r.map((t,a)=>e.jsx("div",{className:`w-3 h-3 rounded-full ${a<l?"bg-green-500":a===l?"bg-purple-500":"bg-slate-200"}`},a))})]}),e.jsx("h3",{className:"text-xl font-bold text-slate-800 mb-6",children:u.question}),e.jsx("div",{className:"space-y-3 mb-6",children:u.options.map((t,a)=>{const n=B===a,m=u.correct===a,x=d;return e.jsx("button",{onClick:()=>!d&&T(a),disabled:d,className:`w-full text-left p-4 rounded-lg border transition-all ${x?m?"bg-green-100 border-green-500 text-green-800":n?"bg-red-100 border-red-500 text-red-800":"bg-slate-50 border-slate-200":n?"bg-purple-100 border-purple-500":"bg-white border-slate-200 hover:bg-slate-50"}`,children:e.jsxs("div",{className:"flex items-center gap-3",children:[x&&m&&e.jsx(W,{className:"text-green-500",size:20}),x&&n&&!m&&e.jsx(D,{className:"text-red-500",size:20}),e.jsx("span",{children:t})]})},a)})}),d&&e.jsx("div",{className:"bg-blue-50 p-4 rounded-lg border border-blue-200 mb-4",children:e.jsxs("div",{className:"flex items-start gap-2",children:[e.jsx(M,{className:"text-blue-500 flex-shrink-0 mt-1",size:18}),e.jsx("p",{className:"text-blue-800",children:u.explanation})]})}),d&&e.jsx("button",{onClick:q,className:"w-full py-3 bg-purple-500 text-white rounded-lg font-medium hover:bg-purple-600",children:l<r.length-1?"Next Question":"See Results"})]})}):e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-white rounded-xl p-6 shadow-lg border",children:[e.jsxs("h3",{className:"text-lg font-bold text-slate-800 mb-4 flex items-center gap-2",children:[e.jsx(I,{size:20,className:"text-purple-500"}),"Configure Model & LoRA"]}),e.jsxs("div",{className:"grid grid-cols-2 gap-4",children:[e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"Model Size (Billion params)"}),e.jsxs("select",{value:s.modelSize,onChange:t=>o({...s,modelSize:Number(t.target.value)}),className:"w-full px-3 py-2 border rounded-lg bg-white",children:[e.jsx("option",{value:1,children:"1B"}),e.jsx("option",{value:7,children:"7B"}),e.jsx("option",{value:13,children:"13B"}),e.jsx("option",{value:33,children:"33B"}),e.jsx("option",{value:65,children:"65B"}),e.jsx("option",{value:70,children:"70B"})]})]}),e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"Hidden Dimension"}),e.jsxs("select",{value:s.hiddenDim,onChange:t=>o({...s,hiddenDim:Number(t.target.value)}),className:"w-full px-3 py-2 border rounded-lg bg-white",children:[e.jsx("option",{value:2048,children:"2048"}),e.jsx("option",{value:4096,children:"4096"}),e.jsx("option",{value:5120,children:"5120"}),e.jsx("option",{value:8192,children:"8192"})]})]}),e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"LoRA Rank (r)"}),e.jsxs("select",{value:s.rank,onChange:t=>o({...s,rank:Number(t.target.value)}),className:"w-full px-3 py-2 border rounded-lg bg-white",children:[e.jsx("option",{value:4,children:"4"}),e.jsx("option",{value:8,children:"8"}),e.jsx("option",{value:16,children:"16"}),e.jsx("option",{value:32,children:"32"}),e.jsx("option",{value:64,children:"64"}),e.jsx("option",{value:128,children:"128"})]})]}),e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"Target Modules"}),e.jsxs("select",{value:s.targetModules,onChange:t=>o({...s,targetModules:Number(t.target.value)}),className:"w-full px-3 py-2 border rounded-lg bg-white",children:[e.jsx("option",{value:2,children:"Q, V only (2)"}),e.jsx("option",{value:4,children:"Q, K, V, O (4)"}),e.jsx("option",{value:6,children:"All attention + MLP (6)"})]})]}),e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"Number of Layers"}),e.jsx("input",{type:"number",value:s.numLayers,onChange:t=>o({...s,numLayers:Number(t.target.value)}),className:"w-full px-3 py-2 border rounded-lg"})]}),e.jsxs("div",{children:[e.jsx("label",{className:"block text-sm font-medium text-slate-700 mb-1",children:"Quantization"}),e.jsxs("select",{value:s.quantization,onChange:t=>o({...s,quantization:t.target.value}),className:"w-full px-3 py-2 border rounded-lg bg-white",children:[e.jsx("option",{value:"none",children:"None (FP16)"}),e.jsx("option",{value:"8bit",children:"8-bit"}),e.jsx("option",{value:"4bit",children:"4-bit (QLoRA)"})]})]})]})]}),e.jsxs("div",{className:"bg-white rounded-xl p-6 shadow-lg border",children:[e.jsxs("h3",{className:"text-lg font-bold text-slate-800 mb-4 flex items-center gap-2",children:[e.jsx(G,{size:20,className:"text-green-500"}),"Memory Estimates"]}),e.jsxs("div",{className:"grid grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-blue-50 p-4 rounded-lg border border-blue-200",children:[e.jsx("div",{className:"text-sm text-blue-600 mb-1",children:"Base Model Memory"}),e.jsxs("div",{className:"text-2xl font-bold text-blue-800",children:[c.baseModel," GB"]})]}),e.jsxs("div",{className:"bg-green-50 p-4 rounded-lg border border-green-200",children:[e.jsx("div",{className:"text-sm text-green-600 mb-1",children:"LoRA Parameters"}),e.jsxs("div",{className:"text-2xl font-bold text-green-800",children:[(c.loraParams/1e6).toFixed(2),"M"]})]}),e.jsxs("div",{className:"bg-purple-50 p-4 rounded-lg border border-purple-200",children:[e.jsx("div",{className:"text-sm text-purple-600 mb-1",children:"LoRA Memory"}),e.jsxs("div",{className:"text-2xl font-bold text-purple-800",children:[c.loraMemory," MB"]})]}),e.jsxs("div",{className:"bg-orange-50 p-4 rounded-lg border border-orange-200",children:[e.jsx("div",{className:"text-sm text-orange-600 mb-1",children:"% Trainable"}),e.jsxs("div",{className:"text-2xl font-bold text-orange-800",children:[c.percentTrainable,"%"]})]})]}),e.jsxs("div",{className:"mt-4 p-4 bg-slate-800 rounded-lg",children:[e.jsx("div",{className:"text-slate-800 dark:text-sm mb-1",children:"Estimated Training Memory"}),e.jsxs("div",{className:"text-3xl font-bold text-white",children:[c.totalTraining," GB"]}),e.jsx("div",{className:"text-slate-800 dark:text-sm mt-1",children:"(includes model + LoRA + optimizer + activations)"})]})]}),e.jsxs("div",{className:"bg-amber-50 p-4 rounded-xl border border-amber-200",children:[e.jsxs("h4",{className:"font-bold text-amber-900 mb-2 flex items-center gap-2",children:[e.jsx(M,{size:18}),"Tips"]}),e.jsxs("ul",{className:"text-sm text-amber-800 space-y-1",children:[e.jsx("li",{children:"â€¢ Start with rank=8 and increase if needed"}),e.jsx("li",{children:"â€¢ QLoRA (4-bit) enables training 7B models on 16GB GPUs"}),e.jsx("li",{children:"â€¢ More target modules = more capacity but more memory"}),e.jsx("li",{children:"â€¢ Actual memory may vary based on batch size and sequence length"})]})]})]})]})})}export{X as default};
